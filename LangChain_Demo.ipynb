{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhrumil84/colabl_langchain/blob/main/LangChain_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVWayIj_1-JY",
        "outputId": "e7e280c1-14a1-41fe-8712-75bb73c025e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.9.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.4\n"
          ]
        }
      ],
      "source": [
        "## Install Dependencies\n",
        "!pip install langchain==0.0.137\n",
        "\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "openai_api_key = \"Enter your OPenAI key here, otherwise the code will not work\""
      ],
      "metadata": {
        "id": "wsjwivc42QQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 : Models - LLM Wrappers"
      ],
      "metadata": {
        "id": "3MthHVd43XMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm=OpenAI(model_name = 'text-davinci-003',openai_api_key=openai_api_key)\n",
        "print(llm(\"explain Indian premier league in 100 words\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrGWeLD-3Q0h",
        "outputId": "8385bf60-ed90-4427-e4e7-30477757a426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Indian Premier League (IPL) is a professional Twenty20 cricket league in India contested during April and May of every year by teams representing Indian cities. It was founded by the Board of Control for Cricket in India (BCCI) in 2008, and is the most-attended cricket league in the world. The IPL is the most-watched cricket league in the world and ranks sixth among all sports leagues. Each team plays against the others twice, once at home and once away, and the top four teams qualify for the playoffs. The IPL culminates with the final match in May, in which the winner is crowned the Indian Premier League champion team.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interacting with Chat Models - GPT-3 or GPT-4"
      ],
      "metadata": {
        "id": "8cZksC-94XJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat models take in 3 parametes - an AI messgae, a Human Message and a System Message to configure the system or the model\n",
        "from langchain.schema import(AIMessage, HumanMessage, SystemMessage)\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "9D5ld57b4WNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To use the chat model, you combine the human message and the system message in a list, and give it as an input to the chat model\n",
        "chat = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature = 0.3,openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "AKiG5xfT8GTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are an expert data scientist\"),\n",
        "    HumanMessage(content=\"Write a Python script to train a neural network on a simulated data\")\n",
        "]\n",
        "response = chat(messages)"
      ],
      "metadata": {
        "id": "Qm0DGIF23lW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content, end=\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lKLTOVe5jZc",
        "outputId": "17e96555-a250-477a-acd9-508efba02b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To train a neural network on simulated data, we will first generate the data, then create a neural network model using a deep learning library like TensorFlow, and finally train the model on the generated data.\n",
            "\n",
            "Here's a Python script to do that:\n",
            "\n",
            "```python\n",
            "import numpy as np\n",
            "import tensorflow as tf\n",
            "from tensorflow.keras.models import Sequential\n",
            "from tensorflow.keras.layers import Dense\n",
            "\n",
            "# Generate simulated data\n",
            "np.random.seed(0)\n",
            "X = np.random.rand(1000, 20)\n",
            "y = np.random.randint(0, 2, size=(1000, 1))\n",
            "\n",
            "# Split data into training and testing sets\n",
            "split_index = int(0.8 * len(X))\n",
            "X_train, X_test = X[:split_index], X[split_index:]\n",
            "y_train, y_test = y[:split_index], y[split_index:]\n",
            "\n",
            "# Create a neural network model\n",
            "model = Sequential()\n",
            "model.add(Dense(64, activation='relu', input_shape=(20,)))\n",
            "model.add(Dense(32, activation='relu'))\n",
            "model.add(Dense(1, activation='sigmoid'))\n",
            "\n",
            "# Compile the model\n",
            "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
            "\n",
            "# Train the model on the simulated data\n",
            "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
            "\n",
            "# Evaluate the model on the test data\n",
            "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
            "print(f'Test accuracy: {test_accuracy:.4f}')\n",
            "```\n",
            "\n",
            "This script generates 1000 samples of simulated data with 20 features each and binary labels. It then creates a simple neural network model with two hidden layers and trains it on the generated data. Finally, it evaluates the model on the test data and prints the test accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 : Prompt Templates\n",
        "\n",
        "Prompt templates are functionality provided by LangChain which takes a piece of text and injects a user input into the text. We can then format the prompt with the user input and feed that to the language model"
      ],
      "metadata": {
        "id": "cbz-wjta58UG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are an expert data scientist with an expertise in building deep learning models.\n",
        "Explain the concept of {concept} in 3 lines\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables = [\"concept\"],\n",
        "    template=template\n",
        ")"
      ],
      "metadata": {
        "id": "HIou5TBF51Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9szSY3F26kwP",
        "outputId": "b8123f4b-ac61-4410-df85-b0c90a1fcd0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['concept'], output_parser=None, partial_variables={}, template='\\nYou are an expert data scientist with an expertise in building deep learning models. \\nExplain the concept of {concept} in 3 lines\\n', template_format='f-string', validate_template=True)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt.format(concept =\"embeddings\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Qrd_fPAX6p45",
        "outputId": "1ad2508c-3105-40a6-8df7-3c5f8ba274d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEmbeddings are low dimensional representations of high dimensional data, such as text or words, that capture the underlying semantics of the data. These representations are learned through neural networks, and can be used as features in various machine learning models for tasks such as text classification and sentiment analysis.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt.format(concept =\"principal component analysis\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_EC1reQb6t2t",
        "outputId": "9536c5c6-0e72-4a85-846e-0ea0635925ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPrincipal Component Analysis (PCA) is an unsupervised dimensionality reduction technique used to reduce the complexity of datasets by extracting the most important features. It works by transforming the dataset into a new coordinate system with reduced dimensions, where the new principal components are created by combining the original variables in such a way that the variance of the data is maximized. PCA can reduce the number of features in a dataset while still retaining the essential information.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt.format(concept =\"autoencoder\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "pNy1L0Yf6yhw",
        "outputId": "6ab351df-ae1b-4cc6-a50f-341bb179a744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAutoencoder is a type of artificial neural network used to learn efficient data representations in an unsupervised manner. It consists of an encoder which maps the input to a latent space, and a decoder which reconstructs the input from the latent space. Autoencoders are used for feature extraction and dimensionality reduction.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Chains\n",
        "\n",
        "A chain is a composite structure that combines an input language model and a prompt template to create an interface. This interface processes user input and generates a response from the language model. Essentially, it works like a composite function with the prompt template as the inner function and the language model as the outer function.\n",
        "\n",
        "Moreover, it is possible to construct sequential chains where the output generated by the first chain is used as input for the second chain, enabling a multi-layered processing system."
      ],
      "metadata": {
        "id": "xsjMeEQu7BqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "##Run the chain specifying only the input variable\n",
        "print(chain.run(\"Autoencoder\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm51paMI8PAf",
        "outputId": "a3dfe29f-2e92-4a0a-e83d-3253df230082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Autoencoders are a type of neural network which are used for unsupervised learning. They learn to compress data using an encoding layer, which is then decoded back to its original form. Autoencoders are used for feature extraction, dimensionality reduction, and anomaly detection.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_prompt = PromptTemplate(\n",
        "    input_variables = [\"ml_concept\"],\n",
        "    template=\"Turn the concept description of {ml_concept} and explain it to me like I am five in 300 words\"\n",
        ")\n",
        "\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ],
      "metadata": {
        "id": "CgSsJl3B66YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Combining the two chains"
      ],
      "metadata": {
        "id": "WgbFV0A78p10"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GV0N4fvC--PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
        "\n",
        "##Run the chain specifying only the input variable for the first chain\n",
        "explanation = overall_chain.run(\"Autoencoder\")\n",
        "print(explanation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvgDjqff8sxd",
        "outputId": "bdde446f-d19e-47e9-dd43-158792dae7b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "Autoencoders are a type of deep learning neural network that take an input and attempt to reconstruct it at the output. They are used for data compression, feature learning, and anomaly detection. Autoencoders are trained using an unsupervised learning algorithm, meaning they learn without labels or targets.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "Autoencoders are a special type of computer program that can take in a bunch of information and try to reconstruct it. For example, if you had a picture of a cat, the autoencoder could take the picture, break it down into its individual pieces, and then put it back together again.\n",
            "\n",
            "Autoencoders can be used to do lots of different things. For example, they can be used to make data smaller, so it takes up less space. They can also be used to learn about different features in the data, like if a picture has a cat in it or not. Finally, they can be used to detect unusual things, like if something doesn’t look quite right.\n",
            "\n",
            "Autoencoders are trained using an unsupervised learning algorithm. This means that they learn without a teacher telling them what to do. Instead, they look at the data and figure out how to process it on their own.\n",
            "\n",
            "In summary, autoencoders are a special type of computer program that can take in information and try to reconstruct it. They can be used for data compression, feature learning, and anomaly detection. They are trained using an unsupervised learning algorithm, meaning they learn without labels or targets\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "Autoencoders are a special type of computer program that can take in a bunch of information and try to reconstruct it. For example, if you had a picture of a cat, the autoencoder could take the picture, break it down into its individual pieces, and then put it back together again.\n",
            "\n",
            "Autoencoders can be used to do lots of different things. For example, they can be used to make data smaller, so it takes up less space. They can also be used to learn about different features in the data, like if a picture has a cat in it or not. Finally, they can be used to detect unusual things, like if something doesn’t look quite right.\n",
            "\n",
            "Autoencoders are trained using an unsupervised learning algorithm. This means that they learn without a teacher telling them what to do. Instead, they look at the data and figure out how to process it on their own.\n",
            "\n",
            "In summary, autoencoders are a special type of computer program that can take in information and try to reconstruct it. They can be used for data compression, feature learning, and anomaly detection. They are trained using an unsupervised learning algorithm, meaning they learn without labels or targets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "caDRCOBa9IiC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CPLRvlHE8-tm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}